model:
  emb_dim: 512
  hidden_dim: 1365
  num_heads: 8
  qkv_bias: true
  max_seq_len: 50
  attn_dropout: 0.0
  transformer_layers: 2
  learning_rate: 1.0e-5
  scheduler_patience: 4
  reduce_lr_by: 0.5
  vocabulary_size: 20

data:
  batch_size: 16
  max_seq_len: 50

trainer:
  max_epochs: 4
  accelerator: auto
  devices: auto
  precision: "16-mixed"
  gradient_clip_val: 1.0
  log_every_n_steps: 50
  val_check_interval: 0.5
  limit_train_batches: 10000
  limit_val_batches: 1000
  
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "models/"
        filename: "model-tune-{epoch:02d}-{val_loss:.3f}"
        monitor: "val_loss"
        mode: "min"
        save_top_k: 2
        save_last: "link"
    
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: "val_loss"
        patience: 2
        mode: "min"
        stopping_threshold: 0.01
        verbose: true

  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: "logs"
        name: "default"
    - class_path: lightning.pytorch.loggers.mlflow.MLFlowLogger
      init_args:
        experiment_name: "tune-config"
        save_dir: "logs-mlflow"