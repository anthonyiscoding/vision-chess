[default]
emb_dim = 1024
batch_size = 30
num_epochs = 2
num_heads = 8
# num_kv_heads, hidden_dim, and head_dim will be set automatically if not defined
# num_kv_heads = 8
# hidden_dim = 2048
# head_dim = 128
qkv_bias = true
max_seq_len = 400  # TODO: figure out reasonable max sequence length
attn_dropout = 0.0
transformer_layers = 6
# batch_limit = 0
save_model = true
learning_rate = 0.000374

[test]
emb_dim = 64
batch_size = 1
num_epochs = 2
num_heads = 8
# num_kv_heads, hidden_dim, and head_dim will be set automatically if not defined
# num_kv_heads = 8
# hidden_dim = 2048
# head_dim = 128
qkv_bias = true
max_seq_len = 400  # TODO: figure out reasonable max sequence length
attn_dropout = 0.0
transformer_layers = 6
# batch_limit = 0
save_model = false
learning_rate = 0.000374

[tune]
emb_dim = 64
batch_size = 2
num_epochs = 1
num_heads = 8
# num_kv_heads, hidden_dim, and head_dim will be set automatically if not defined
# num_kv_heads = 8
# hidden_dim = 2048
# head_dim = 128
qkv_bias = true
max_seq_len = 400  # TODO: figure out reasonable max sequence length
attn_dropout = 0.0
transformer_layers = 6
batch_limit = 250
save_model = false
learning_rate = 0.000374