[default]
emb_dim = 1024
batch_size = 16
num_epochs = 2
num_heads = 8
# num_kv_heads, hidden_dim, and head_dim will be set automatically if not defined
# num_kv_heads = 8
# hidden_dim = 2048
# head_dim = 128
qkv_bias = true
max_seq_len = 200  # TODO: figure out reasonable max sequence length
attn_dropout = 0.0
transformer_layers = 8
# batch_limit = 0
save_model = true
learning_rate = 1e-4

[test]
emb_dim = 64
batch_size = 1
num_epochs = 2
num_heads = 8
qkv_bias = true
max_seq_len = 200
attn_dropout = 0.0
transformer_layers = 6
batch_limit = 10
save_model = true
learning_rate = 1e-4

[tune]
emb_dim = 64
batch_size = 2
num_epochs = 1
num_heads = 8
qkv_bias = true
max_seq_len = 200
attn_dropout = 0.0
transformer_layers = 6
batch_limit = 250
save_model = false
learning_rate = 1e-4