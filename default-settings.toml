[default]
emb_dim = 512
hidden_dim = 1365
batch_size = 512
num_epochs = 16
num_heads = 8
qkv_bias = true
# TODO: figure out reasonable max sequence length
max_seq_len = 50
attn_dropout = 0.0
transformer_layers = 2
# batch_limit = 4_000_000
save_model = true
learning_rate = 7e-5

[test]
emb_dim = 64
batch_size = 1
num_epochs = 2
num_heads = 8
qkv_bias = true
max_seq_len = 50
attn_dropout = 0.0
transformer_layers = 6
batch_limit = 10
save_model = false
learning_rate = 1e-4
# fast_dev_run = 10

[tune]
emb_dim = 512
batch_size = 16
num_epochs = 4
num_heads = 8
qkv_bias = true
max_seq_len = 50
attn_dropout = 0.0
transformer_layers = 2
batch_limit = 10_000
save_model = true
learning_rate = 1e-4